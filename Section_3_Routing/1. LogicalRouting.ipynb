{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "209d276a",
   "metadata": {},
   "source": [
    "# HyDE (Hypothetical Document Embeddings) Demo\n",
    "\n",
    "HyDE improves retrieval by using an LLM to generate hypothetical documents that would answer a user's question. Rather than directly retrieving documents based on the original query, HyDE first generates a relevant passage that could answer the question, then uses that generated passage for retrieval. This approach better aligns the query semantics with the document space and can improve retrieval quality.\n",
    "\n",
    "## What this notebook contains\n",
    "- Using an LLM to generate hypothetical documents that answer a given question.\n",
    "- Creating embeddings from the generated documents and storing them in a vector database.\n",
    "- Retrieving relevant documents using the generated hypothetical documents.\n",
    "- Building a RAG pipeline that answers the question using the retrieved context.\n",
    "\n",
    "## References\n",
    "https://arxiv.org/abs/2212.10496"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1a240d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "/Users/bolinpan/Library/Python/3.13/lib/python/site-packages/IPython/core/interactiveshell.py:3699: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  \n",
    "from langchain_community.document_loaders import WebBaseLoader  \n",
    "from langchain_community.vectorstores import Chroma  \n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser  \n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings \n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.load import dumps, loads\n",
    "from langchain.schema import Document\n",
    "from typing import Literal\n",
    "import yaml\n",
    "import bs4  \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "090b5a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Build the path to config.yaml\n",
    "config_path = os.path.join(cwd, '..', 'configs', 'config.yaml')\n",
    "\n",
    "# Normalize the path\n",
    "config_path = os.path.abspath(config_path)\n",
    "\n",
    "# Load credential from config file\n",
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['LANGCHAIN_API_KEY'] = config['API']['LANGCHAIN']\n",
    "os.environ['OPENAI_API_KEY'] = config['API']['OPENAI']\n",
    "\n",
    "# Configure chat LLM (deterministic)\n",
    "llm = ChatOpenAI(temperature=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57d6d8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a loader that fetches and parses the target web page\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),  # tuple of URLs to load\n",
    "    bs_kwargs=dict(  # pass BeautifulSoup-specific kwargs to limit parsing\n",
    "        parse_only=bs4.SoupStrainer(  # only parse these parts of the page to reduce noise\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Fetch and return a list of Document objects\n",
    "docs = loader.load()  \n",
    "\n",
    "# Split long documents into smaller overlapping chunks suitable for embeddings\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "splits = text_splitter.split_documents(docs)  # list of smaller document chunks\n",
    "\n",
    "# Create embeddings and store them in a vector DB (Chroma)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())  # uses OpenAI embeddings under the hood\n",
    "\n",
    "# Create a retriever to fetch relevant docs (return the top 1 results)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e5c46e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:1900: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:1913: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Data model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"\n",
    "    Route a user query to the most relevant datasource.\n",
    "    \"\"\"\n",
    "\n",
    "    datasource: Literal[\"python_docs\", \"js_docs\", \"golang_docs\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose which datasource would be most relevant for answering their question\",\n",
    "    )\n",
    "\n",
    "# LLM with function call \n",
    "structured_llm = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"You are an expert at routing a user question to the appropriate data source.\n",
    "\n",
    "Based on the programming language the question is referring to, route it to the relevant data source.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define router \n",
    "router = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b69826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define question and check pydantic result\n",
    "question = \"\"\"Why doesn't the following code work:\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
    "prompt.invoke(\"french\")\n",
    "\"\"\"\n",
    "\n",
    "result = router.invoke({\"question\": question})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9843a9cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RouteQuery(datasource='python_docs')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Define a branch that uses result.datasource\n",
    "def choose_route(result):\n",
    "    if \"python_docs\" in result.datasource.lower():\n",
    "        ### Logic here \n",
    "        return \"chain for python_docs\"\n",
    "    elif \"js_docs\" in result.datasource.lower():\n",
    "        ### Logic here \n",
    "        return \"chain for js_docs\"\n",
    "    else:\n",
    "        ### Logic here \n",
    "        return \"golang_docs\"\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "full_chain = router | RunnableLambda(choose_route)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
