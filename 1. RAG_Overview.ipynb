{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "209d276a",
   "metadata": {},
   "source": [
    "# RAG (Retrieval-Augmented Generation) Overview\n",
    "\n",
    "This notebook demonstrates a minimal Retrieval-Augmented Generation (RAG) pipeline using LangChain components and OpenAI models.\n",
    "The goal is to: ingest content from a web page, split it into chunks, embed those chunks into a vector store, and then build a retriever + prompt + LLM chain to answer questions using retrieved context.\n",
    "\n",
    "## What this notebook does\n",
    "- Loads and parses a target web article using `WebBaseLoader` and BeautifulSoup.\n",
    "- Splits long text into overlapping chunks with `RecursiveCharacterTextSplitter` for robust retrieval.\n",
    "- Creates embeddings for each chunk using `OpenAIEmbeddings` and stores them in a `Chroma` vector store.\n",
    "- Builds a retriever from the vector store to fetch context relevant to a query.\n",
    "- Pulls a prompt template from the LangChain Hub (`rlm/rag-prompt`) and composes a small runnable chain: retriever -> prompt -> LLM -> output parser.\n",
    "- Demonstrates running the chain with a sample question.\n",
    "\n",
    "## Notes and next steps\n",
    "- You can replace `Chroma` with another vector store backend as needed.\n",
    "- Adjust `chunk_size` / `chunk_overlap` depending on the document lengths and token budget.\n",
    "- For production, consider caching, error handling, and an async flow for larger corpora.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1a240d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub  \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  \n",
    "from langchain_community.document_loaders import WebBaseLoader  \n",
    "from langchain_community.vectorstores import Chroma  \n",
    "from langchain_core.output_parsers import StrOutputParser  \n",
    "from langchain_core.runnables import RunnablePassthrough  \n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings \n",
    "import yaml\n",
    "import bs4  \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "090b5a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credential from config file\n",
    "with open('configs/config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = config['API']['LANGCHAIN']\n",
    "os.environ['OPENAI_API_KEY'] = config['API']['OPENAI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8827d182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a loader that fetches and parses the target web page\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),  # tuple of URLs to load\n",
    "    bs_kwargs=dict(  # pass BeautifulSoup-specific kwargs to limit parsing\n",
    "        parse_only=bs4.SoupStrainer(  # only parse these parts of the page to reduce noise\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Fetch and return a list of Document objects\n",
    "docs = loader.load()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0248d225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split long documents into smaller overlapping chunks suitable for embeddings\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)  # list of smaller Document chunks\n",
    "\n",
    "# Create embeddings and store them in a vector DB (Chroma)\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings())  # uses OpenAI embeddings under the hood\n",
    "\n",
    "# Create a retriever to fetch relevant docs given a query\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c9f2fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: {question} \n",
      "Context: {context} \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "# Load a prompt template from the LangChain hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Print the prompt template\n",
    "print(prompt.messages[0].prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7427fc47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task Decomposition is a technique used to break down complex tasks into smaller and simpler steps, allowing for easier execution and understanding. It can be achieved through methods such as prompting with specific instructions, utilizing external classical planners, or incorporating human inputs. By decomposing tasks, models can effectively manage and interpret the thinking process involved in completing a task.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure the chat LLM to use for generation\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Helper to format retrieved documents into a single context string\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)  # join each doc with blank line separators\n",
    "\n",
    "# Wire up retriever -> prompt -> llm -> parser as a runnable chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}  # map inputs: context comes from retriever + formatter, question passes through\n",
    "    | prompt  # inject prompt template\n",
    "    | llm  # call the language model\n",
    "    | StrOutputParser()  # ensure final output is a plain string\n",
    ")\n",
    "\n",
    "# Run the chain with a test question (synchronous invocation)\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")  # returns model output as a string"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
