{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "209d276a",
   "metadata": {},
   "source": [
    "# HyDE (Hypothetical Document Embeddings) Demo\n",
    "\n",
    "HyDE improves retrieval by using an LLM to generate hypothetical documents that would answer a user's question. Rather than directly retrieving documents based on the original query, HyDE first generates a relevant passage that could answer the question, then uses that generated passage for retrieval. This approach better aligns the query semantics with the document space and can improve retrieval quality.\n",
    "\n",
    "## What this notebook contains\n",
    "- Using an LLM to generate hypothetical documents that answer a given question.\n",
    "- Creating embeddings from the generated documents and storing them in a vector database.\n",
    "- Retrieving relevant documents using the generated hypothetical documents.\n",
    "- Building a RAG pipeline that answers the question using the retrieved context.\n",
    "\n",
    "## References\n",
    "https://arxiv.org/abs/2212.10496"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1a240d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  \n",
    "from langchain_community.document_loaders import WebBaseLoader  \n",
    "from langchain_community.vectorstores import Chroma  \n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser  \n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings \n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.load import dumps, loads\n",
    "from langchain.schema import Document\n",
    "import yaml\n",
    "import bs4  \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "090b5a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Build the path to config.yaml\n",
    "config_path = os.path.join(cwd, '..', 'configs', 'config.yaml')\n",
    "\n",
    "# Normalize the path\n",
    "config_path = os.path.abspath(config_path)\n",
    "\n",
    "# Load credential from config file\n",
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['LANGCHAIN_API_KEY'] = config['API']['LANGCHAIN']\n",
    "os.environ['OPENAI_API_KEY'] = config['API']['OPENAI']\n",
    "\n",
    "# Configure chat LLM (deterministic)\n",
    "llm = ChatOpenAI(temperature=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57d6d8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a loader that fetches and parses the target web page\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),  # tuple of URLs to load\n",
    "    bs_kwargs=dict(  # pass BeautifulSoup-specific kwargs to limit parsing\n",
    "        parse_only=bs4.SoupStrainer(  # only parse these parts of the page to reduce noise\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Fetch and return a list of Document objects\n",
    "docs = loader.load()  \n",
    "\n",
    "# Split long documents into smaller overlapping chunks suitable for embeddings\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "splits = text_splitter.split_documents(docs)  # list of smaller document chunks\n",
    "\n",
    "# Create embeddings and store them in a vector DB (Chroma)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())  # uses OpenAI embeddings under the hood\n",
    "\n",
    "# Create a retriever to fetch relevant docs (return the top 1 results)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aeb2785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyDE document generation\n",
    "template = \"\"\"Please write a scientific paper passage to answer the question\n",
    "Question: {question}\n",
    "Passage:\"\"\"\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Build the HyDE chain\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde | llm | StrOutputParser() \n",
    ")\n",
    "\n",
    "# Run\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generated_docs = generate_docs_for_retrieval.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe35e006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wrap strings into Document objects\n",
    "docs = [Document(page_content=text) for text in generated_docs]\n",
    "\n",
    "# Split long documents into smaller overlapping chunks suitable for embeddings\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "splits = text_splitter.split_documents(docs)  # list of smaller document chunks\n",
    "\n",
    "# Create embeddings and store them in a vector DB (Chroma)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())  # uses OpenAI embeddings under the hood\n",
    "\n",
    "# Create a retriever to fetch relevant docs (return the top 1 results)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "# Retrieve\n",
    "retrieval_chain = generate_docs_for_retrieval | retriever \n",
    "retrieved_docs = retrieval_chain.invoke({\"question\":question})\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22b69826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents involves breaking down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG - build and run final chain that answers the question using retrieved context\n",
    "template = \"\"\"\n",
    "Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Build the prompt object\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Compose pipeline: retrieval_chain -> prompt -> llm -> parse\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()  # parse final output to string\n",
    ")\n",
    "\n",
    "# Execute the chain and get the answer\n",
    "final_rag_chain.invoke({\"context\":retrieved_docs,\"question\":question})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
