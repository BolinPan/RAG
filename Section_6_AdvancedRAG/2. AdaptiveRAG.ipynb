{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe7ebd5d",
   "metadata": {},
   "source": [
    "# Document Reranking Demo\n",
    "\n",
    "Document reranking improves retrieval quality by using a specialized reranking model to reorder initial retrieval results. Rather than relying solely on semantic similarity from vector embeddings, reranking applies a more sophisticated relevance model (such as Cohere's rerank model) to score retrieved documents and return only the most relevant ones. This post-processing step refines results and reduces noise in the retrieved context.\n",
    "\n",
    "## What this notebook contains\n",
    "- Loading and chunking documents from web sources.\n",
    "- Creating a vector database with OpenAI embeddings for initial retrieval.\n",
    "- Setting up a base retriever to fetch top-k candidate documents.\n",
    "- Using Cohere's reranking model to score and reorder retrieved documents.\n",
    "- Building a `ContextualCompressionRetriever` that combines initial retrieval with reranking.\n",
    "- Executing queries to retrieve and rerank documents based on relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78335843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  \n",
    "from langchain_community.document_loaders import WebBaseLoader  \n",
    "from langchain_community.vectorstores import Chroma  \n",
    "from langchain_core.output_parsers import StrOutputParser  \n",
    "from langchain_core.runnables import RunnablePassthrough  \n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings \n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.load import dumps, loads\n",
    "from langchain_community.llms import Cohere\n",
    "from langchain.retrievers import  ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "import numpy as np\n",
    "import yaml\n",
    "import bs4  \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eda644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Build the path to config.yaml\n",
    "config_path = os.path.join(cwd, '..', 'configs', 'config.yaml')\n",
    "\n",
    "# Normalize the path\n",
    "config_path = os.path.abspath(config_path)\n",
    "\n",
    "# Load credential from config file\n",
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['LANGCHAIN_API_KEY'] = config['API']['LANGCHAIN']\n",
    "os.environ['OPENAI_API_KEY'] = config['API']['OPENAI']\n",
    "os.environ['TAVILY_API_KEY'] = config['API']['TAVILY']\n",
    "\n",
    "# Configure chat LLM (deterministic)\n",
    "llm = ChatOpenAI(temperature=0) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
