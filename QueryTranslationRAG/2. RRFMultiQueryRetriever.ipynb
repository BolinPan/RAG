{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "209d276a",
   "metadata": {},
   "source": [
    "# Reciprocal Rank Fusion (RRF) + Multi-Query Retrieval Demo\n",
    "\n",
    "This notebook demonstrates a retrieval fusion pattern that combines multiple retrieval results using Reciprocal Rank Fusion (RRF) to produce a consolidated ranking of candidate documents. The goal is to improve retrieval robustness by aggregating results from multiple query variants generated by an LLM.\n",
    "\n",
    "## What this notebook contains\n",
    "- Generating multiple query variants from a single user question using a Chat LLM.\n",
    "- Retrieving documents for each variant using a vector store (Chroma) and OpenAI embeddings.\n",
    "- Applying Reciprocal Rank Fusion (RRF) to merge ranked result lists.\n",
    "- Selecting top candidates and using them with a RAG prompt to produce a final answer.\n",
    "\n",
    "## References\n",
    "https://towardsdatascience.com/how-to-make-your-llm-more-accurate-with-rag-fine-tuning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1a240d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  \n",
    "from langchain_community.document_loaders import WebBaseLoader  \n",
    "from langchain_community.vectorstores import Chroma  \n",
    "from langchain_core.output_parsers import StrOutputParser  \n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings \n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.load import dumps, loads\n",
    "from operator import itemgetter\n",
    "import yaml\n",
    "import bs4  \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "090b5a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Build the path to config.yaml\n",
    "config_path = os.path.join(cwd, '..', 'configs', 'config.yaml')\n",
    "\n",
    "# Normalize the path\n",
    "config_path = os.path.abspath(config_path)\n",
    "\n",
    "# Load credential from config file\n",
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Set environment variables\n",
    "os.environ['LANGCHAIN_API_KEY'] = config['API']['LANGCHAIN']\n",
    "os.environ['OPENAI_API_KEY'] = config['API']['OPENAI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8827d182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a loader that fetches and parses the target web page\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),  # tuple of URLs to load\n",
    "    bs_kwargs=dict(  # pass BeautifulSoup-specific kwargs to limit parsing\n",
    "        parse_only=bs4.SoupStrainer(  # only parse these parts of the page to reduce noise\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Fetch and return a list of Document objects\n",
    "docs = loader.load()  \n",
    "\n",
    "# Split long documents into smaller overlapping chunks suitable for embeddings\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "splits = text_splitter.split_documents(docs)  # list of smaller Document chunks\n",
    "\n",
    "# Create embeddings and store them in a vector DB (Chroma)\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings())  # uses OpenAI embeddings under the hood\n",
    "\n",
    "# Create a retriever to fetch relevant docs (return the top 5 results)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c9f2fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask an LLM to rephrase the question into multiple variants (generate mult-queries)\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to:  {question} \\n\n",
    "Output (5 queries):\n",
    "\"\"\"\n",
    "\n",
    "# Build the prompt object\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Pipeline: prompt -> chat model -> parse -> split into separate queries\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | ChatOpenAI(temperature=0) \n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))  # split on newlines into a list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd4b9465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. How do LLM agents use task decomposition in problem-solving?',\n",
       " '2. Benefits of task decomposition for LLM agents in artificial intelligence.',\n",
       " '3. Examples of task decomposition strategies for LLM agents.',\n",
       " '4. Challenges of implementing task decomposition for LLM agents.',\n",
       " '5. Comparison of task decomposition approaches for LLM agents in different AI systems.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example input question\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "\n",
    "# Generate multiple query variants\n",
    "result = generate_queries.invoke({\"question\":question})  \n",
    "\n",
    "# Display the generated variants\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805adb7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='The system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Challenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:')],\n",
       " [Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Overview of a LLM-powered autonomous agent system.'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='over complicated task content; (3) Stability improvement of LLM outputs and external model services.'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#')],\n",
       " [Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='The system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Overview of a LLM-powered autonomous agent system.'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#')],\n",
       " [Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='}\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Challenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='The system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='over complicated task content; (3) Stability improvement of LLM outputs and external model services.'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.')],\n",
       " [Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Overview of a LLM-powered autonomous agent system.'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='The system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='}\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:'),\n",
       "  Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.')]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compose the multi-query retrieval pipeline and run it (generate -> retrieve)\n",
    "simple_retrieval_chain = generate_queries | retriever.map() \n",
    "\n",
    "# Retrieve unique docs\n",
    "docs = simple_retrieval_chain.invoke({\"question\":question}) \n",
    "\n",
    "# Show unique retrieved documents\n",
    "display(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7427fc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2n/7qg7n4m55nn6p0lx1w8bgmqh0000gn/T/ipykernel_56383/175969847.py:24: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  (loads(doc), score)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.'),\n",
       "  0.08042274705581157),\n",
       " (Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='The system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.'),\n",
       "  0.06558258417063283),\n",
       " (Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'),\n",
       "  0.064814141547682),\n",
       " (Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Overview of a LLM-powered autonomous agent system.'),\n",
       "  0.04893312516263336),\n",
       " (Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='}\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:'),\n",
       "  0.032539682539682535),\n",
       " (Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Challenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.'),\n",
       "  0.03252247488101534),\n",
       " (Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='over complicated task content; (3) Stability improvement of LLM outputs and external model services.'),\n",
       "  0.031746031746031744),\n",
       " (Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#'),\n",
       "  0.03125),\n",
       " (Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:'),\n",
       "  0.015625)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='The system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " # RRF merges multiple ranked result lists into one fused ranking\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" \n",
    "    Reciprocal_rank_fusion that takes multiple lists of ranked documents and an optional parameter k used in the RRF formula.\n",
    "    - Ranks are assigned within each list, e.g. rank 0 for 1st document, rank 1 for 2nd document, etc.\n",
    "    - The fused score for each document is calculated across all lists.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "    display(reranked_results)\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "# Select the top k candidate from RRF \n",
    "def top_k_docs(docs, k=3): \n",
    "    \"\"\"\n",
    "    Select the top k document from the RRF results\n",
    "    \"\"\"\n",
    "    # take top-k documents\n",
    "    top_k_docs = [doc for doc, score in docs[:k]]\n",
    "\n",
    "    # return list[Document]\n",
    "    return top_k_docs\n",
    "\n",
    "retrieval_chain = generate_queries | retriever.map() | reciprocal_rank_fusion | top_k_docs\n",
    "docs = retrieval_chain.invoke({\"question\": question})\n",
    "\n",
    "# Show unique retrieved documents  # present the final selected docs\n",
    "display(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "901023b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.'),\n",
       "  0.08042274705581157),\n",
       " (Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='The system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.'),\n",
       "  0.06585580821434867),\n",
       " (Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'),\n",
       "  0.048915917503966164),\n",
       " (Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Overview of a LLM-powered autonomous agent system.'),\n",
       "  0.04868510928961749),\n",
       " (Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Challenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.'),\n",
       "  0.04814747488101534),\n",
       " (Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='}\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:'),\n",
       "  0.032539682539682535),\n",
       " (Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='over complicated task content; (3) Stability improvement of LLM outputs and external model services.'),\n",
       "  0.031746031746031744),\n",
       " (Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='(2) Model selection: LLM distributes the tasks to expert models, where the request is framed as a multiple-choice question. LLM is presented with a list of models to choose from. Due to the limited context length, task type based filtration is needed.\\nInstruction:'),\n",
       "  0.03149801587301587),\n",
       " (Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#'),\n",
       "  0.015625)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents involves breaking down large tasks into smaller, manageable subgoals through methods such as simple prompting, task-specific instructions, or human inputs.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG - build and run final chain that answers the question using retrieved context\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# Build the prompt object\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Configure chat LLM (deterministic)\n",
    "llm = ChatOpenAI(temperature=0) \n",
    "\n",
    "# Compose pipeline: retrieval_chain -> prompt -> llm -> parse\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()  # parse final output to string\n",
    ")\n",
    "\n",
    "# Execute the chain and get the answer\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
